{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6lYnCJnYO2g"
      },
      "source": [
        "1Ô∏è‚É£ Data Loader (data_loader.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNxTQ3hZYF0A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class CrowdDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "      #def __init__(self, root_dir: str, transform: Optional[Callable] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Path to the ShanghaiTech dataset.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        #def __init__(self, root_dir: str, transform: Optional[Callable] = None):\n",
        "\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.image_paths = [os.path.join(root_dir, \"images\", img)\n",
        "                            for img in os.listdir(os.path.join(root_dir, \"images\"))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        gt_path = img_path.replace(\"images\", \"ground_truth\").replace(\".jpg\", \".h5\")\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        with h5py.File(gt_path, \"r\") as hf:\n",
        "            target = np.asarray(hf[\"density\"])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdTxmELeYpSQ"
      },
      "source": [
        "2Ô∏è‚É£ CSRNet Model (model.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPn2-KZuYrH4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class CSRNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CSRNet, self).__init__()\n",
        "        vgg = models.vgg16_bn(pretrained=True)\n",
        "\n",
        "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33])\n",
        "        self.backend = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(),\n",
        "            nn.Conv2d(512, 256, 3, padding=2, dilation=2), nn.ReLU(),\n",
        "            nn.Conv2d(256, 128, 3, padding=2, dilation=2), nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, 3, padding=2, dilation=2), nn.ReLU(),\n",
        "            nn.Conv2d(64, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.frontend(x)\n",
        "        x = self.backend(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WklhypZyYxHC"
      },
      "source": [
        "3Ô∏è‚É£ Training Script (train.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx-V_gXxY1vm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class CSRNet(nn.Module):\n",
        "    def __init__(self, load_weights=True):\n",
        "        super(CSRNet, self).__init__()\n",
        "        vgg = models.vgg16_bn(pretrained=load_weights)\n",
        "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33])\n",
        "\n",
        "        self.backend = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.frontend(x)\n",
        "        x = self.backend(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9LVaK40ZGEZ"
      },
      "source": [
        "*4Ô∏è‚É£* Real-time Inference + Alerts (infer_realtime.py)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgR0IccSZLxe",
        "outputId": "b3edd2e3-8c2b-4ee3-8e61-9bcae5e7a79a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CSRNet(nn.Module):\n",
        "    def __init__(self, load_weights=True):\n",
        "        super(CSRNet, self).__init__()\n",
        "        vgg = models.vgg16_bn(pretrained=load_weights)\n",
        "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33])\n",
        "        self.backend = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.frontend(x)\n",
        "        x = self.backend(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIEdB3pZgYH"
      },
      "source": [
        "5Ô∏è‚É£ Streamlit Dashboard (dashboard.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP3zHot6ZhKY",
        "outputId": "ba93abe9-6932-4de2-8cbe-43e53f22e088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSelecting previously unselected package cloudflared.\n",
            "(Reading database ... 126435 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
            "Unpacking cloudflared (2025.9.0) ...\n",
            "Setting up cloudflared (2025.9.0) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[90m2025-09-18T12:17:24Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-09-18T12:17:24Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m |  https://documentary-raid-leg-dropped.trycloudflare.com                                    |\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.9.0 (Checksum 4f48c9fc205ed7d200a35f24170e795171189b77a696e3f8b5704d07a6801957)\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update if installed by a package manager.\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: ec97cadb-2a1e-4483-afdf-2496038a861d\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67\n",
            "2025/09/18 12:17:28 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-09-18T12:17:28Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m1ecc4ba6-cc88-4092-890d-1a1ec52aee19 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67 \u001b[36mlocation=\u001b[0matl01 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-09-18T12:19:59Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2025-09-18T12:20:00Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67\n",
            "\u001b[90m2025-09-18T12:20:00Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67\n",
            "\u001b[90m2025-09-18T12:20:00Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67\n",
            "\u001b[90m2025-09-18T12:20:00Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.67\n",
            "\u001b[90m2025-09-18T12:20:00Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2025-09-18T12:20:00Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2025-09-18T12:20:00Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2025-09-18T12:20:00Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install streamlit -q\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "\n",
        "app_code = \"\"\"\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "st.title(\"CSRNet Crowd Counting Demo (Colab + Cloudflare)\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Display image\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "    # Dummy crowd count (replace with CSRNet later)\n",
        "    st.success(f\"Estimated Crowd Count: {np.random.randint(50,500)}\")\n",
        "else:\n",
        "    st.info(\"Please upload an image to start crowd counting.\")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "\n",
        "!streamlit run app.py &>/dev/null&\n",
        "\n",
        "import time; time.sleep(5)\n",
        "\n",
        "!cloudflared tunnel --url http://localhost:8501 --no-autoupdate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6Ô∏è‚É£ Install Required Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install h5py opencv-python matplotlib tqdm scipy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7Ô∏è‚É£ Generate Ground Truth Density Maps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import h5py\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "def create_density_map(image_shape, points, sigma=15):\n",
        "    \"\"\"Create density map from point annotations\"\"\"\n",
        "    density_map = np.zeros(image_shape, dtype=np.float32)\n",
        "    \n",
        "    for point in points:\n",
        "        x, y = int(point[0]), int(point[1])\n",
        "        if 0 <= x < image_shape[1] and 0 <= y < image_shape[0]:\n",
        "            density_map[y, x] = 1.0\n",
        "    \n",
        "    # Apply Gaussian filter\n",
        "    density_map = gaussian_filter(density_map, sigma=sigma)\n",
        "    return density_map\n",
        "\n",
        "def generate_synthetic_annotations(image_path, num_people_range=(10, 100)):\n",
        "    \"\"\"Generate synthetic point annotations for training\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width = image.shape[:2]\n",
        "    \n",
        "    # Generate random points (simulating people locations)\n",
        "    num_people = np.random.randint(num_people_range[0], num_people_range[1])\n",
        "    points = []\n",
        "    \n",
        "    for _ in range(num_people):\n",
        "        x = np.random.randint(0, width)\n",
        "        y = np.random.randint(0, height)\n",
        "        points.append([x, y])\n",
        "    \n",
        "    return points\n",
        "\n",
        "def process_dataset(images_dir, output_dir):\n",
        "    \"\"\"Process all images and generate ground truth density maps\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, \"ground_truth\"), exist_ok=True)\n",
        "    \n",
        "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    \n",
        "    print(f\"Processing {len(image_files)} images...\")\n",
        "    \n",
        "    for i, img_file in enumerate(image_files):\n",
        "        img_path = os.path.join(images_dir, img_file)\n",
        "        \n",
        "        # Load image to get dimensions\n",
        "        image = cv2.imread(img_path)\n",
        "        height, width = image.shape[:2]\n",
        "        \n",
        "        # Generate synthetic annotations\n",
        "        points = generate_synthetic_annotations(img_path)\n",
        "        \n",
        "        # Create density map\n",
        "        density_map = create_density_map((height, width), points)\n",
        "        \n",
        "        # Save density map as HDF5\n",
        "        gt_filename = img_file.replace('.jpg', '.h5').replace('.jpeg', '.h5').replace('.png', '.h5')\n",
        "        gt_path = os.path.join(output_dir, \"ground_truth\", gt_filename)\n",
        "        \n",
        "        with h5py.File(gt_path, 'w') as hf:\n",
        "            hf['density'] = density_map\n",
        "        \n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Processed {i + 1}/{len(image_files)} images\")\n",
        "    \n",
        "    print(\"Ground truth generation completed!\")\n",
        "\n",
        "# Process your images dataset\n",
        "images_dir = \"images\"\n",
        "output_dir = \"dataset\"\n",
        "\n",
        "if os.path.exists(images_dir):\n",
        "    process_dataset(images_dir, output_dir)\n",
        "    print(f\"‚úÖ Ground truth generated in '{output_dir}' directory\")\n",
        "else:\n",
        "    print(f\"‚ùå Images directory '{images_dir}' not found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8Ô∏è‚É£ Create Data Loader for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class CrowdDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, train=True):\n",
        "        \"\"\"Dataset for crowd counting\"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "        \n",
        "        # Get all image files\n",
        "        images_dir = os.path.join(root_dir, \"images\")\n",
        "        if not os.path.exists(images_dir):\n",
        "            images_dir = root_dir  # If images are directly in root_dir\n",
        "        \n",
        "        self.image_paths = []\n",
        "        for img_file in os.listdir(images_dir):\n",
        "            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                self.image_paths.append(os.path.join(images_dir, img_file))\n",
        "        \n",
        "        # Split into train/validation (80/20 split)\n",
        "        np.random.seed(42)\n",
        "        indices = np.random.permutation(len(self.image_paths))\n",
        "        split_idx = int(0.8 * len(self.image_paths))\n",
        "        \n",
        "        if train:\n",
        "            self.image_paths = [self.image_paths[i] for i in indices[:split_idx]]\n",
        "        else:\n",
        "            self.image_paths = [self.image_paths[i] for i in indices[split_idx:]]\n",
        "        \n",
        "        print(f\"{'Training' if train else 'Validation'} dataset: {len(self.image_paths)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        \n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        # Get corresponding ground truth path\n",
        "        img_name = os.path.basename(img_path)\n",
        "        gt_name = img_name.replace('.jpg', '.h5').replace('.jpeg', '.h5').replace('.png', '.h5')\n",
        "        gt_path = os.path.join(self.root_dir, \"ground_truth\", gt_name)\n",
        "        \n",
        "        # Load density map\n",
        "        if os.path.exists(gt_path):\n",
        "            with h5py.File(gt_path, \"r\") as hf:\n",
        "                target = np.asarray(hf[\"density\"])\n",
        "        else:\n",
        "            # If no ground truth exists, create a dummy one\n",
        "            target = np.zeros((image.size[1], image.size[0]), dtype=np.float32)\n",
        "            print(f\"Warning: No ground truth found for {img_name}\")\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        # Convert target to tensor\n",
        "        target = torch.from_numpy(target).float()\n",
        "        \n",
        "        return image, target\n",
        "\n",
        "def get_transforms():\n",
        "    \"\"\"Get data augmentation transforms for training and validation\"\"\"\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    return train_transform, val_transform\n",
        "\n",
        "def create_data_loaders(root_dir, batch_size=8, num_workers=4):\n",
        "    \"\"\"Create training and validation data loaders\"\"\"\n",
        "    train_transform, val_transform = get_transforms()\n",
        "    \n",
        "    train_dataset = CrowdDataset(root_dir, transform=train_transform, train=True)\n",
        "    val_dataset = CrowdDataset(root_dir, transform=val_transform, train=False)\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True, \n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False, \n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    return train_loader, val_loader\n",
        "\n",
        "# Test the data loader\n",
        "root_dir = \"dataset\"\n",
        "if os.path.exists(root_dir):\n",
        "    train_loader, val_loader = create_data_loaders(root_dir, batch_size=4)\n",
        "    print(\"‚úÖ Data loaders created successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå Dataset directory not found. Please run the ground truth generation cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "9Ô∏è‚É£ Training Script - Complete Training Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CrowdCountingLoss(nn.Module):\n",
        "    \"\"\"Custom loss function for crowd counting\"\"\"\n",
        "    def __init__(self):\n",
        "        super(CrowdCountingLoss, self).__init__()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        # Ensure target has the same spatial dimensions as prediction\n",
        "        if pred.shape != target.shape:\n",
        "            target = torch.nn.functional.interpolate(\n",
        "                target.unsqueeze(1), \n",
        "                size=pred.shape[2:], \n",
        "                mode='bilinear', \n",
        "                align_corners=False\n",
        "            ).squeeze(1)\n",
        "        \n",
        "        return self.mse_loss(pred, target)\n",
        "\n",
        "def calculate_mae_rmse(pred, target):\n",
        "    \"\"\"Calculate Mean Absolute Error and Root Mean Square Error\"\"\"\n",
        "    pred_count = torch.sum(pred, dim=(1, 2, 3))\n",
        "    target_count = torch.sum(target, dim=(1, 2, 3))\n",
        "    \n",
        "    mae = torch.mean(torch.abs(pred_count - target_count))\n",
        "    rmse = torch.sqrt(torch.mean((pred_count - target_count) ** 2))\n",
        "    \n",
        "    return mae.item(), rmse.item()\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_mae = 0\n",
        "    total_rmse = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch} [Train]')\n",
        "    \n",
        "    for batch_idx, (images, targets) in enumerate(pbar):\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Calculate metrics\n",
        "        mae, rmse = calculate_mae_rmse(outputs, targets)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_mae += mae\n",
        "        total_rmse += rmse\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'MAE': f'{mae:.2f}',\n",
        "            'RMSE': f'{rmse:.2f}'\n",
        "        })\n",
        "    \n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_mae = total_mae / len(train_loader)\n",
        "    avg_rmse = total_rmse / len(train_loader)\n",
        "    \n",
        "    return avg_loss, avg_mae, avg_rmse\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device, epoch):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_mae = 0\n",
        "    total_rmse = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc=f'Epoch {epoch} [Val]')\n",
        "        \n",
        "        for batch_idx, (images, targets) in enumerate(pbar):\n",
        "            images, targets = images.to(device), targets.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            # Calculate metrics\n",
        "            mae, rmse = calculate_mae_rmse(outputs, targets)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            total_mae += mae\n",
        "            total_rmse += rmse\n",
        "            \n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'MAE': f'{mae:.2f}',\n",
        "                'RMSE': f'{rmse:.2f}'\n",
        "            })\n",
        "    \n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    avg_mae = total_mae / len(val_loader)\n",
        "    avg_rmse = total_rmse / len(val_loader)\n",
        "    \n",
        "    return avg_loss, avg_mae, avg_rmse\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "\n",
        "print(\"‚úÖ Training functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üîü Start Training the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "config = {\n",
        "    'batch_size': 8,\n",
        "    'learning_rate': 1e-4,\n",
        "    'num_epochs': 50,  # Reduced for faster training\n",
        "    'num_workers': 2,\n",
        "    'dataset_path': 'dataset',\n",
        "    'checkpoint_dir': 'checkpoints',\n",
        "    'save_interval': 10,\n",
        "    'patience': 15  # Early stopping patience\n",
        "}\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Using device: {device}\")\n",
        "\n",
        "# Create data loaders\n",
        "print(\"üìä Loading dataset...\")\n",
        "train_loader, val_loader = create_data_loaders(\n",
        "    config['dataset_path'], \n",
        "    config['batch_size'], \n",
        "    config['num_workers']\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "print(\"üß† Initializing CSRNet model...\")\n",
        "model = CSRNet(load_weights=True).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = CrowdCountingLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n",
        "\n",
        "# Training history\n",
        "train_losses, val_losses = [], []\n",
        "train_maes, val_maes = [], []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"üéØ Starting training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, config['num_epochs'] + 1):\n",
        "    # Train\n",
        "    train_loss, train_mae, train_rmse = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, device, epoch\n",
        "    )\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_mae, val_rmse = validate_epoch(\n",
        "        model, val_loader, criterion, device, epoch\n",
        "    )\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Store history\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_maes.append(train_mae)\n",
        "    val_maes.append(val_mae)\n",
        "    \n",
        "    # Print epoch summary\n",
        "    print(f'Epoch {epoch}/{config[\"num_epochs\"]}:')\n",
        "    print(f'  Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.2f}, Train RMSE: {train_rmse:.2f}')\n",
        "    print(f'  Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.2f}, Val RMSE: {val_rmse:.2f}')\n",
        "    print(f'  Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "    print('-' * 50)\n",
        "    \n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        save_checkpoint(\n",
        "            model, optimizer, epoch, val_loss,\n",
        "            os.path.join(config['checkpoint_dir'], 'best_model.pth')\n",
        "        )\n",
        "        print(f'‚úÖ New best model saved! Val Loss: {val_loss:.4f}')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    \n",
        "    # Save checkpoint at intervals\n",
        "    if epoch % config['save_interval'] == 0:\n",
        "        save_checkpoint(\n",
        "            model, optimizer, epoch, val_loss,\n",
        "            os.path.join(config['checkpoint_dir'], f'checkpoint_epoch_{epoch}.pth')\n",
        "        )\n",
        "    \n",
        "    # Early stopping\n",
        "    if patience_counter >= config['patience']:\n",
        "        print(f'üõë Early stopping triggered after {epoch} epochs')\n",
        "        break\n",
        "\n",
        "# Save final model\n",
        "save_checkpoint(\n",
        "    model, optimizer, epoch, val_loss,\n",
        "    os.path.join(config['checkpoint_dir'], 'final_model.pth')\n",
        ")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"=\"*60)\n",
        "print(f'üéâ Training completed in {total_time/3600:.2f} hours')\n",
        "print(f'üèÜ Best validation loss: {best_val_loss:.4f}')\n",
        "print(f'üíæ Models saved in: {config[\"checkpoint_dir\"]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1Ô∏è‚É£1Ô∏è‚É£ Plot Training Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "def plot_training_history(train_losses, val_losses, train_maes, val_maes):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Plot losses\n",
        "    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
        "    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot MAE\n",
        "    ax2.plot(epochs, train_maes, 'b-', label='Training MAE', linewidth=2)\n",
        "    ax2.plot(epochs, val_maes, 'r-', label='Validation MAE', linewidth=2)\n",
        "    ax2.set_title('Training and Validation MAE', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('MAE')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the results\n",
        "if len(train_losses) > 0:\n",
        "    plot_training_history(train_losses, val_losses, train_maes, val_maes)\n",
        "    print(\"üìä Training plots saved as 'training_history.png'\")\n",
        "else:\n",
        "    print(\"‚ùå No training data to plot. Please run the training cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1Ô∏è‚É£2Ô∏è‚É£ Test the Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best trained model and test it\n",
        "def load_model_for_inference(checkpoint_path, device):\n",
        "    \"\"\"Load trained model for inference\"\"\"\n",
        "    model = CSRNet(load_weights=False).to(device)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def test_single_image(model, image_path, device):\n",
        "    \"\"\"Test model on a single image\"\"\"\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    original_size = image.size\n",
        "    \n",
        "    # Apply transforms (same as validation)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        density_map = model(input_tensor)\n",
        "        count = torch.sum(density_map).item()\n",
        "    \n",
        "    # Convert density map to numpy\n",
        "    density_map = density_map.squeeze().cpu().numpy()\n",
        "    \n",
        "    return {\n",
        "        'count': count,\n",
        "        'density_map': density_map,\n",
        "        'original_image': image,\n",
        "        'original_size': original_size\n",
        "    }\n",
        "\n",
        "def visualize_prediction(result, image_name):\n",
        "    \"\"\"Visualize prediction results\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # Original image\n",
        "    axes[0].imshow(result['original_image'])\n",
        "    axes[0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Density map\n",
        "    im = axes[1].imshow(result['density_map'], cmap='hot')\n",
        "    axes[1].set_title(f'Density Map (Count: {result[\"count\"]:.1f})', fontsize=12, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "    \n",
        "    # Overlay\n",
        "    overlay = result['original_image'].copy()\n",
        "    density_resized = cv2.resize(result['density_map'], result['original_size'])\n",
        "    density_resized = (density_resized - density_resized.min()) / (density_resized.max() - density_resized.min())\n",
        "    \n",
        "    # Create colored overlay\n",
        "    heatmap = plt.cm.hot(density_resized)[:, :, :3]\n",
        "    heatmap = (heatmap * 255).astype(np.uint8)\n",
        "    \n",
        "    # Blend images\n",
        "    blended = cv2.addWeighted(np.array(overlay), 0.7, heatmap, 0.3, 0)\n",
        "    \n",
        "    axes[2].imshow(blended)\n",
        "    axes[2].set_title(f'Overlay (Count: {result[\"count\"]:.1f})', fontsize=12, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    plt.suptitle(f'CSRNet Prediction: {image_name}', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test the model\n",
        "checkpoint_path = \"checkpoints/best_model.pth\"\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(\"üéØ Loading trained model...\")\n",
        "    trained_model = load_model_for_inference(checkpoint_path, device)\n",
        "    \n",
        "    # Test on a few sample images\n",
        "    test_images = []\n",
        "    if os.path.exists(\"images\"):\n",
        "        image_files = [f for f in os.listdir(\"images\") if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        test_images = [os.path.join(\"images\", f) for f in image_files[:3]]  # Test first 3 images\n",
        "    \n",
        "    if test_images:\n",
        "        print(f\"üß™ Testing on {len(test_images)} sample images...\")\n",
        "        for i, img_path in enumerate(test_images):\n",
        "            print(f\"\\n--- Testing Image {i+1}: {os.path.basename(img_path)} ---\")\n",
        "            result = test_single_image(trained_model, img_path, device)\n",
        "            print(f\"Predicted crowd count: {result['count']:.1f}\")\n",
        "            visualize_prediction(result, os.path.basename(img_path))\n",
        "    else:\n",
        "        print(\"‚ùå No test images found in 'images' directory\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå Trained model not found. Please run the training cell first.\")\n",
        "    print(f\"Expected checkpoint at: {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6oorO52vv7H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
